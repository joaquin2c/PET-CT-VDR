{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "67b8fa80-8964-4e92-bd6d-b9d0e6856040",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "import itertools\n",
    "from collections.abc import Sequence\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.checkpoint as checkpoint\n",
    "from torch.nn import LayerNorm\n",
    "from typing_extensions import Final\n",
    "\n",
    "from monai.networks.blocks import MLPBlock as Mlp\n",
    "from monai.networks.blocks import PatchEmbed, UnetOutBlock, UnetrBasicBlock, UnetrUpBlock\n",
    "from monai.networks.layers import DropPath, trunc_normal_, get_act_layer\n",
    "from monai.utils import ensure_tuple_rep, look_up_option, optional_import\n",
    "from monai.utils.deprecate_utils import deprecated_arg\n",
    "\n",
    "rearrange, _ = optional_import(\"einops\", name=\"rearrange\")\n",
    "\n",
    "\n",
    "SUPPORTED_DROPOUT_MODE = {\"vit\", \"swin\", \"vista3d\"}\n",
    "\n",
    "__all__ = [\n",
    "    \"window_partition\",\n",
    "    \"window_reverse\",\n",
    "    \"WindowAttention\",\n",
    "    \"SwinTransformerBlock\",\n",
    "    \"PatchMerging\",\n",
    "    \"PatchMergingV2\",\n",
    "    \"MERGING_MODE\",\n",
    "    \"BasicLayer\",\n",
    "    \"SwinTransformer\",\n",
    "]\n",
    "\n",
    "def window_partition(x, window_size):\n",
    "    \"\"\"window partition operation based on: \"Liu et al.,\n",
    "    Swin Transformer: Hierarchical Vision Transformer using Shifted Windows\n",
    "    <https://arxiv.org/abs/2103.14030>\"\n",
    "    https://github.com/microsoft/Swin-Transformer\n",
    "\n",
    "     Args:\n",
    "        x: input tensor.\n",
    "        window_size: local window size.\n",
    "    \"\"\"\n",
    "    x_shape = x.size()  # length 4 or 5 only\n",
    "    if len(x_shape) == 5:\n",
    "        b, d, h, w, c = x_shape\n",
    "        x = x.view(\n",
    "            b,\n",
    "            d // window_size[0],\n",
    "            window_size[0],\n",
    "            h // window_size[1],\n",
    "            window_size[1],\n",
    "            w // window_size[2],\n",
    "            window_size[2],\n",
    "            c,\n",
    "        )\n",
    "        windows = (\n",
    "            x.permute(0, 1, 3, 5, 2, 4, 6, 7).contiguous().view(-1, window_size[0] * window_size[1] * window_size[2], c)\n",
    "        )\n",
    "    else:  # if len(x_shape) == 4:\n",
    "        b, h, w, c = x.shape\n",
    "        x = x.view(b, h // window_size[0], window_size[0], w // window_size[1], window_size[1], c)\n",
    "        windows = x.permute(0, 1, 3, 2, 4, 5).contiguous().view(-1, window_size[0] * window_size[1], c)\n",
    "\n",
    "    return windows\n",
    "\n",
    "\n",
    "def window_reverse(windows, window_size, dims):\n",
    "    \"\"\"window reverse operation based on: \"Liu et al.,\n",
    "    Swin Transformer: Hierarchical Vision Transformer using Shifted Windows\n",
    "    <https://arxiv.org/abs/2103.14030>\"\n",
    "    https://github.com/microsoft/Swin-Transformer\n",
    "\n",
    "     Args:\n",
    "        windows: windows tensor.\n",
    "        window_size: local window size.\n",
    "        dims: dimension values.\n",
    "    \"\"\"\n",
    "    if len(dims) == 4:\n",
    "        b, d, h, w = dims\n",
    "        x = windows.view(\n",
    "            b,\n",
    "            d // window_size[0],\n",
    "            h // window_size[1],\n",
    "            w // window_size[2],\n",
    "            window_size[0],\n",
    "            window_size[1],\n",
    "            window_size[2],\n",
    "            -1,\n",
    "        )\n",
    "        x = x.permute(0, 1, 4, 2, 5, 3, 6, 7).contiguous().view(b, d, h, w, -1)\n",
    "\n",
    "    elif len(dims) == 3:\n",
    "        b, h, w = dims\n",
    "        x = windows.view(b, h // window_size[0], w // window_size[1], window_size[0], window_size[1], -1)\n",
    "        x = x.permute(0, 1, 3, 2, 4, 5).contiguous().view(b, h, w, -1)\n",
    "    return x\n",
    "\n",
    "\n",
    "def get_window_size(x_size, window_size, shift_size=None):\n",
    "    \"\"\"Computing window size based on: \"Liu et al.,\n",
    "    Swin Transformer: Hierarchical Vision Transformer using Shifted Windows\n",
    "    <https://arxiv.org/abs/2103.14030>\"\n",
    "    https://github.com/microsoft/Swin-Transformer\n",
    "\n",
    "     Args:\n",
    "        x_size: input size.\n",
    "        window_size: local window size.\n",
    "        shift_size: window shifting size.\n",
    "    \"\"\"\n",
    "\n",
    "    use_window_size = list(window_size)\n",
    "    if shift_size is not None:\n",
    "        use_shift_size = list(shift_size)\n",
    "    for i in range(len(x_size)):\n",
    "        if x_size[i] <= window_size[i]:\n",
    "            use_window_size[i] = x_size[i]\n",
    "            if shift_size is not None:\n",
    "                use_shift_size[i] = 0\n",
    "\n",
    "    if shift_size is None:\n",
    "        return tuple(use_window_size)\n",
    "    else:\n",
    "        return tuple(use_window_size), tuple(use_shift_size)\n",
    "\n",
    "\n",
    "class WindowAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Window based multi-head self attention module with relative position bias based on: \"Liu et al.,\n",
    "    Swin Transformer: Hierarchical Vision Transformer using Shifted Windows\n",
    "    <https://arxiv.org/abs/2103.14030>\"\n",
    "    https://github.com/microsoft/Swin-Transformer\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim: int,\n",
    "        num_heads: int,\n",
    "        window_size: Sequence[int],\n",
    "        qkv_bias: bool = False,\n",
    "        attn_drop: float = 0.0,\n",
    "        proj_drop: float = 0.0,\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            dim: number of feature channels.\n",
    "            num_heads: number of attention heads.\n",
    "            window_size: local window size.\n",
    "            qkv_bias: add a learnable bias to query, key, value.\n",
    "            attn_drop: attention dropout rate.\n",
    "            proj_drop: dropout rate of output.\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.window_size = window_size\n",
    "        self.num_heads = num_heads\n",
    "        head_dim = dim // num_heads\n",
    "        self.scale = head_dim**-0.5\n",
    "        mesh_args = torch.meshgrid.__kwdefaults__\n",
    "\n",
    "        if len(self.window_size) == 3:\n",
    "            self.relative_position_bias_table = nn.Parameter(\n",
    "                torch.zeros(\n",
    "                    (2 * self.window_size[0] - 1) * (2 * self.window_size[1] - 1) * (2 * self.window_size[2] - 1),\n",
    "                    num_heads,\n",
    "                )\n",
    "            )\n",
    "            coords_d = torch.arange(self.window_size[0])\n",
    "            coords_h = torch.arange(self.window_size[1])\n",
    "            coords_w = torch.arange(self.window_size[2])\n",
    "            if mesh_args is not None:\n",
    "                coords = torch.stack(torch.meshgrid(coords_d, coords_h, coords_w, indexing=\"ij\"))\n",
    "            else:\n",
    "                coords = torch.stack(torch.meshgrid(coords_d, coords_h, coords_w))\n",
    "            coords_flatten = torch.flatten(coords, 1)\n",
    "            relative_coords = coords_flatten[:, :, None] - coords_flatten[:, None, :]\n",
    "            relative_coords = relative_coords.permute(1, 2, 0).contiguous()\n",
    "            relative_coords[:, :, 0] += self.window_size[0] - 1\n",
    "            relative_coords[:, :, 1] += self.window_size[1] - 1\n",
    "            relative_coords[:, :, 2] += self.window_size[2] - 1\n",
    "            relative_coords[:, :, 0] *= (2 * self.window_size[1] - 1) * (2 * self.window_size[2] - 1)\n",
    "            relative_coords[:, :, 1] *= 2 * self.window_size[2] - 1\n",
    "        elif len(self.window_size) == 2:\n",
    "            self.relative_position_bias_table = nn.Parameter(\n",
    "                torch.zeros((2 * window_size[0] - 1) * (2 * window_size[1] - 1), num_heads)\n",
    "            )\n",
    "            coords_h = torch.arange(self.window_size[0])\n",
    "            coords_w = torch.arange(self.window_size[1])\n",
    "            if mesh_args is not None:\n",
    "                coords = torch.stack(torch.meshgrid(coords_h, coords_w, indexing=\"ij\"))\n",
    "            else:\n",
    "                coords = torch.stack(torch.meshgrid(coords_h, coords_w))\n",
    "            coords_flatten = torch.flatten(coords, 1)\n",
    "            relative_coords = coords_flatten[:, :, None] - coords_flatten[:, None, :]\n",
    "            relative_coords = relative_coords.permute(1, 2, 0).contiguous()\n",
    "            relative_coords[:, :, 0] += self.window_size[0] - 1\n",
    "            relative_coords[:, :, 1] += self.window_size[1] - 1\n",
    "            relative_coords[:, :, 0] *= 2 * self.window_size[1] - 1\n",
    "\n",
    "        relative_position_index = relative_coords.sum(-1)\n",
    "        self.register_buffer(\"relative_position_index\", relative_position_index)\n",
    "        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n",
    "        self.attn_drop = nn.Dropout(attn_drop)\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "        self.proj_drop = nn.Dropout(proj_drop)\n",
    "        trunc_normal_(self.relative_position_bias_table, std=0.02)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        b, n, c = x.shape\n",
    "        qkv = self.qkv(x).reshape(b, n, 3, self.num_heads, c // self.num_heads).permute(2, 0, 3, 1, 4)\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
    "        q = q * self.scale\n",
    "        attn = q @ k.transpose(-2, -1)\n",
    "        relative_position_bias = self.relative_position_bias_table[\n",
    "            self.relative_position_index.clone()[:n, :n].reshape(-1)\n",
    "        ].reshape(n, n, -1)\n",
    "        relative_position_bias = relative_position_bias.permute(2, 0, 1).contiguous()\n",
    "        attn = attn + relative_position_bias.unsqueeze(0)\n",
    "        if mask is not None:\n",
    "            nw = mask.shape[0]\n",
    "            attn = attn.view(b // nw, nw, self.num_heads, n, n) + mask.unsqueeze(1).unsqueeze(0)\n",
    "            attn = attn.view(-1, self.num_heads, n, n)\n",
    "            attn = self.softmax(attn)\n",
    "        else:\n",
    "            attn = self.softmax(attn)\n",
    "\n",
    "        attn = self.attn_drop(attn).to(v.dtype)\n",
    "        x = (attn @ v).transpose(1, 2).reshape(b, n, c)\n",
    "        x = self.proj(x)\n",
    "        x = self.proj_drop(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class SwinTransformerBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Swin Transformer block based on: \"Liu et al.,\n",
    "    Swin Transformer: Hierarchical Vision Transformer using Shifted Windows\n",
    "    <https://arxiv.org/abs/2103.14030>\"\n",
    "    https://github.com/microsoft/Swin-Transformer\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim: int,\n",
    "        num_heads: int,\n",
    "        window_size: Sequence[int],\n",
    "        shift_size: Sequence[int],\n",
    "        mlp_ratio: float = 4.0,\n",
    "        qkv_bias: bool = True,\n",
    "        drop: float = 0.0,\n",
    "        attn_drop: float = 0.0,\n",
    "        drop_path: float = 0.0,\n",
    "        act_layer: str = \"GELU\",\n",
    "        norm_layer: type[LayerNorm] = nn.LayerNorm,\n",
    "        use_checkpoint: bool = False,\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            dim: number of feature channels.\n",
    "            num_heads: number of attention heads.\n",
    "            window_size: local window size.\n",
    "            shift_size: window shift size.\n",
    "            mlp_ratio: ratio of mlp hidden dim to embedding dim.\n",
    "            qkv_bias: add a learnable bias to query, key, value.\n",
    "            drop: dropout rate.\n",
    "            attn_drop: attention dropout rate.\n",
    "            drop_path: stochastic depth rate.\n",
    "            act_layer: activation layer.\n",
    "            norm_layer: normalization layer.\n",
    "            use_checkpoint: use gradient checkpointing for reduced memory usage.\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.num_heads = num_heads\n",
    "        self.window_size = window_size\n",
    "        self.shift_size = shift_size\n",
    "        self.mlp_ratio = mlp_ratio\n",
    "        self.use_checkpoint = use_checkpoint\n",
    "        self.norm1 = norm_layer(dim)\n",
    "        self.attn = WindowAttention(\n",
    "            dim,\n",
    "            window_size=self.window_size,\n",
    "            num_heads=num_heads,\n",
    "            qkv_bias=qkv_bias,\n",
    "            attn_drop=attn_drop,\n",
    "            proj_drop=drop,\n",
    "        )\n",
    "\n",
    "        self.drop_path = DropPath(drop_path) if drop_path > 0.0 else nn.Identity()\n",
    "        self.norm2 = norm_layer(dim)\n",
    "        mlp_hidden_dim = int(dim * mlp_ratio)\n",
    "        self.mlp = Mlp(hidden_size=dim, mlp_dim=mlp_hidden_dim, act=act_layer, dropout_rate=drop, dropout_mode=\"swin\")\n",
    "\n",
    "    def forward_part1(self, x, mask_matrix):\n",
    "        x_shape = x.size()\n",
    "        x = self.norm1(x)\n",
    "        if len(x_shape) == 5:\n",
    "            b, d, h, w, c = x.shape\n",
    "            window_size, shift_size = get_window_size((d, h, w), self.window_size, self.shift_size)\n",
    "            pad_l = pad_t = pad_d0 = 0\n",
    "            pad_d1 = (window_size[0] - d % window_size[0]) % window_size[0]\n",
    "            pad_b = (window_size[1] - h % window_size[1]) % window_size[1]\n",
    "            pad_r = (window_size[2] - w % window_size[2]) % window_size[2]\n",
    "            x = F.pad(x, (0, 0, pad_l, pad_r, pad_t, pad_b, pad_d0, pad_d1))\n",
    "            _, dp, hp, wp, _ = x.shape\n",
    "            dims = [b, dp, hp, wp]\n",
    "\n",
    "        else:  # elif len(x_shape) == 4\n",
    "            b, h, w, c = x.shape\n",
    "            window_size, shift_size = get_window_size((h, w), self.window_size, self.shift_size)\n",
    "            pad_l = pad_t = 0\n",
    "            pad_b = (window_size[0] - h % window_size[0]) % window_size[0]\n",
    "            pad_r = (window_size[1] - w % window_size[1]) % window_size[1]\n",
    "            x = F.pad(x, (0, 0, pad_l, pad_r, pad_t, pad_b))\n",
    "            _, hp, wp, _ = x.shape\n",
    "            dims = [b, hp, wp]\n",
    "\n",
    "        if any(i > 0 for i in shift_size):\n",
    "            if len(x_shape) == 5:\n",
    "                shifted_x = torch.roll(x, shifts=(-shift_size[0], -shift_size[1], -shift_size[2]), dims=(1, 2, 3))\n",
    "            elif len(x_shape) == 4:\n",
    "                shifted_x = torch.roll(x, shifts=(-shift_size[0], -shift_size[1]), dims=(1, 2))\n",
    "            attn_mask = mask_matrix\n",
    "        else:\n",
    "            shifted_x = x\n",
    "            attn_mask = None\n",
    "        x_windows = window_partition(shifted_x, window_size)\n",
    "        attn_windows = self.attn(x_windows, mask=attn_mask)\n",
    "        attn_windows = attn_windows.view(-1, *(window_size + (c,)))\n",
    "        shifted_x = window_reverse(attn_windows, window_size, dims)\n",
    "        if any(i > 0 for i in shift_size):\n",
    "            if len(x_shape) == 5:\n",
    "                x = torch.roll(shifted_x, shifts=(shift_size[0], shift_size[1], shift_size[2]), dims=(1, 2, 3))\n",
    "            elif len(x_shape) == 4:\n",
    "                x = torch.roll(shifted_x, shifts=(shift_size[0], shift_size[1]), dims=(1, 2))\n",
    "        else:\n",
    "            x = shifted_x\n",
    "\n",
    "        if len(x_shape) == 5:\n",
    "            if pad_d1 > 0 or pad_r > 0 or pad_b > 0:\n",
    "                x = x[:, :d, :h, :w, :].contiguous()\n",
    "        elif len(x_shape) == 4:\n",
    "            if pad_r > 0 or pad_b > 0:\n",
    "                x = x[:, :h, :w, :].contiguous()\n",
    "\n",
    "        return x\n",
    "\n",
    "    def forward_part2(self, x):\n",
    "        return self.drop_path(self.mlp(self.norm2(x)))\n",
    "\n",
    "    def load_from(self, weights, n_block, layer):\n",
    "        root = f\"module.{layer}.0.blocks.{n_block}.\"\n",
    "        block_names = [\n",
    "            \"norm1.weight\",\n",
    "            \"norm1.bias\",\n",
    "            \"attn.relative_position_bias_table\",\n",
    "            \"attn.relative_position_index\",\n",
    "            \"attn.qkv.weight\",\n",
    "            \"attn.qkv.bias\",\n",
    "            \"attn.proj.weight\",\n",
    "            \"attn.proj.bias\",\n",
    "            \"norm2.weight\",\n",
    "            \"norm2.bias\",\n",
    "            \"mlp.fc1.weight\",\n",
    "            \"mlp.fc1.bias\",\n",
    "            \"mlp.fc2.weight\",\n",
    "            \"mlp.fc2.bias\",\n",
    "        ]\n",
    "        with torch.no_grad():\n",
    "            self.norm1.weight.copy_(weights[\"state_dict\"][root + block_names[0]])\n",
    "            self.norm1.bias.copy_(weights[\"state_dict\"][root + block_names[1]])\n",
    "            self.attn.relative_position_bias_table.copy_(weights[\"state_dict\"][root + block_names[2]])\n",
    "            self.attn.relative_position_index.copy_(weights[\"state_dict\"][root + block_names[3]])\n",
    "            self.attn.qkv.weight.copy_(weights[\"state_dict\"][root + block_names[4]])\n",
    "            self.attn.qkv.bias.copy_(weights[\"state_dict\"][root + block_names[5]])\n",
    "            self.attn.proj.weight.copy_(weights[\"state_dict\"][root + block_names[6]])\n",
    "            self.attn.proj.bias.copy_(weights[\"state_dict\"][root + block_names[7]])\n",
    "            self.norm2.weight.copy_(weights[\"state_dict\"][root + block_names[8]])\n",
    "            self.norm2.bias.copy_(weights[\"state_dict\"][root + block_names[9]])\n",
    "            self.mlp.linear1.weight.copy_(weights[\"state_dict\"][root + block_names[10]])\n",
    "            self.mlp.linear1.bias.copy_(weights[\"state_dict\"][root + block_names[11]])\n",
    "            self.mlp.linear2.weight.copy_(weights[\"state_dict\"][root + block_names[12]])\n",
    "            self.mlp.linear2.bias.copy_(weights[\"state_dict\"][root + block_names[13]])\n",
    "\n",
    "    def forward(self, x, mask_matrix):\n",
    "        shortcut = x\n",
    "        if self.use_checkpoint:\n",
    "            x = checkpoint.checkpoint(self.forward_part1, x, mask_matrix, use_reentrant=False)\n",
    "        else:\n",
    "            x = self.forward_part1(x, mask_matrix)\n",
    "        x = shortcut + self.drop_path(x)\n",
    "        if self.use_checkpoint:\n",
    "            x = x + checkpoint.checkpoint(self.forward_part2, x, use_reentrant=False)\n",
    "        else:\n",
    "            x = x + self.forward_part2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class PatchMergingV2(nn.Module):\n",
    "    \"\"\"\n",
    "    Patch merging layer based on: \"Liu et al.,\n",
    "    Swin Transformer: Hierarchical Vision Transformer using Shifted Windows\n",
    "    <https://arxiv.org/abs/2103.14030>\"\n",
    "    https://github.com/microsoft/Swin-Transformer\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dim: int, norm_layer: type[LayerNorm] = nn.LayerNorm, spatial_dims: int = 3) -> None:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            dim: number of feature channels.\n",
    "            norm_layer: normalization layer.\n",
    "            spatial_dims: number of spatial dims.\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        if spatial_dims == 3:\n",
    "            self.reduction = nn.Linear(8 * dim, 2 * dim, bias=False)\n",
    "            self.norm = norm_layer(8 * dim)\n",
    "        elif spatial_dims == 2:\n",
    "            self.reduction = nn.Linear(4 * dim, 2 * dim, bias=False)\n",
    "            self.norm = norm_layer(4 * dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_shape = x.size()\n",
    "        if len(x_shape) == 5:\n",
    "            b, d, h, w, c = x_shape\n",
    "            pad_input = (h % 2 == 1) or (w % 2 == 1) or (d % 2 == 1)\n",
    "            if pad_input:\n",
    "                x = F.pad(x, (0, 0, 0, w % 2, 0, h % 2, 0, d % 2))\n",
    "            x = torch.cat(\n",
    "                [x[:, i::2, j::2, k::2, :] for i, j, k in itertools.product(range(2), range(2), range(2))], -1\n",
    "            )\n",
    "\n",
    "        elif len(x_shape) == 4:\n",
    "            b, h, w, c = x_shape\n",
    "            pad_input = (h % 2 == 1) or (w % 2 == 1)\n",
    "            if pad_input:\n",
    "                x = F.pad(x, (0, 0, 0, w % 2, 0, h % 2))\n",
    "            x = torch.cat([x[:, j::2, i::2, :] for i, j in itertools.product(range(2), range(2))], -1)\n",
    "\n",
    "        x = self.norm(x)\n",
    "        x = self.reduction(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class PatchMerging(PatchMergingV2):\n",
    "    \"\"\"The `PatchMerging` module previously defined in v0.9.0.\"\"\"\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_shape = x.size()\n",
    "        if len(x_shape) == 4:\n",
    "            return super().forward(x)\n",
    "        if len(x_shape) != 5:\n",
    "            raise ValueError(f\"expecting 5D x, got {x.shape}.\")\n",
    "        b, d, h, w, c = x_shape\n",
    "        pad_input = (h % 2 == 1) or (w % 2 == 1) or (d % 2 == 1)\n",
    "        if pad_input:\n",
    "            x = F.pad(x, (0, 0, 0, w % 2, 0, h % 2, 0, d % 2))\n",
    "        x0 = x[:, 0::2, 0::2, 0::2, :]\n",
    "        x1 = x[:, 1::2, 0::2, 0::2, :]\n",
    "        x2 = x[:, 0::2, 1::2, 0::2, :]\n",
    "        x3 = x[:, 0::2, 0::2, 1::2, :]\n",
    "        x4 = x[:, 1::2, 0::2, 1::2, :]\n",
    "        x5 = x[:, 0::2, 1::2, 0::2, :]\n",
    "        x6 = x[:, 0::2, 0::2, 1::2, :]\n",
    "        x7 = x[:, 1::2, 1::2, 1::2, :]\n",
    "        x = torch.cat([x0, x1, x2, x3, x4, x5, x6, x7], -1)\n",
    "        x = self.norm(x)\n",
    "        x = self.reduction(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "MERGING_MODE = {\"merging\": PatchMerging, \"mergingv2\": PatchMergingV2}\n",
    "\n",
    "\n",
    "def compute_mask(dims, window_size, shift_size, device):\n",
    "    \"\"\"Computing region masks based on: \"Liu et al.,\n",
    "    Swin Transformer: Hierarchical Vision Transformer using Shifted Windows\n",
    "    <https://arxiv.org/abs/2103.14030>\"\n",
    "    https://github.com/microsoft/Swin-Transformer\n",
    "\n",
    "     Args:\n",
    "        dims: dimension values.\n",
    "        window_size: local window size.\n",
    "        shift_size: shift size.\n",
    "        device: device.\n",
    "    \"\"\"\n",
    "\n",
    "    cnt = 0\n",
    "\n",
    "    if len(dims) == 3:\n",
    "        d, h, w = dims\n",
    "        img_mask = torch.zeros((1, d, h, w, 1), device=device)\n",
    "        for d in slice(-window_size[0]), slice(-window_size[0], -shift_size[0]), slice(-shift_size[0], None):\n",
    "            for h in slice(-window_size[1]), slice(-window_size[1], -shift_size[1]), slice(-shift_size[1], None):\n",
    "                for w in slice(-window_size[2]), slice(-window_size[2], -shift_size[2]), slice(-shift_size[2], None):\n",
    "                    img_mask[:, d, h, w, :] = cnt\n",
    "                    cnt += 1\n",
    "\n",
    "    elif len(dims) == 2:\n",
    "        h, w = dims\n",
    "        img_mask = torch.zeros((1, h, w, 1), device=device)\n",
    "        for h in slice(-window_size[0]), slice(-window_size[0], -shift_size[0]), slice(-shift_size[0], None):\n",
    "            for w in slice(-window_size[1]), slice(-window_size[1], -shift_size[1]), slice(-shift_size[1], None):\n",
    "                img_mask[:, h, w, :] = cnt\n",
    "                cnt += 1\n",
    "\n",
    "    mask_windows = window_partition(img_mask, window_size)\n",
    "    mask_windows = mask_windows.squeeze(-1)\n",
    "    attn_mask = mask_windows.unsqueeze(1) - mask_windows.unsqueeze(2)\n",
    "    attn_mask = attn_mask.masked_fill(attn_mask != 0, float(-100.0)).masked_fill(attn_mask == 0, float(0.0))\n",
    "\n",
    "    return attn_mask\n",
    "\n",
    "\n",
    "class BasicLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Basic Swin Transformer layer in one stage based on: \"Liu et al.,\n",
    "    Swin Transformer: Hierarchical Vision Transformer using Shifted Windows\n",
    "    <https://arxiv.org/abs/2103.14030>\"\n",
    "    https://github.com/microsoft/Swin-Transformer\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim: int,\n",
    "        depth: int,\n",
    "        num_heads: int,\n",
    "        window_size: Sequence[int],\n",
    "        drop_path: list,\n",
    "        mlp_ratio: float = 4.0,\n",
    "        qkv_bias: bool = False,\n",
    "        drop: float = 0.0,\n",
    "        attn_drop: float = 0.0,\n",
    "        norm_layer: type[LayerNorm] = nn.LayerNorm,\n",
    "        downsample: nn.Module | None = None,\n",
    "        use_checkpoint: bool = False,\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            dim: number of feature channels.\n",
    "            depth: number of layers in each stage.\n",
    "            num_heads: number of attention heads.\n",
    "            window_size: local window size.\n",
    "            drop_path: stochastic depth rate.\n",
    "            mlp_ratio: ratio of mlp hidden dim to embedding dim.\n",
    "            qkv_bias: add a learnable bias to query, key, value.\n",
    "            drop: dropout rate.\n",
    "            attn_drop: attention dropout rate.\n",
    "            norm_layer: normalization layer.\n",
    "            downsample: an optional downsampling layer at the end of the layer.\n",
    "            use_checkpoint: use gradient checkpointing for reduced memory usage.\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "        self.window_size = window_size\n",
    "        self.shift_size = tuple(i // 2 for i in window_size)\n",
    "        self.no_shift = tuple(0 for i in window_size)\n",
    "        self.depth = depth\n",
    "        self.use_checkpoint = use_checkpoint\n",
    "        self.blocks = nn.ModuleList(\n",
    "            [\n",
    "                SwinTransformerBlock(\n",
    "                    dim=dim,\n",
    "                    num_heads=num_heads,\n",
    "                    window_size=self.window_size,\n",
    "                    shift_size=self.no_shift if (i % 2 == 0) else self.shift_size,\n",
    "                    mlp_ratio=mlp_ratio,\n",
    "                    qkv_bias=qkv_bias,\n",
    "                    drop=drop,\n",
    "                    attn_drop=attn_drop,\n",
    "                    drop_path=drop_path[i] if isinstance(drop_path, list) else drop_path,\n",
    "                    norm_layer=norm_layer,\n",
    "                    use_checkpoint=use_checkpoint,\n",
    "                )\n",
    "                for i in range(depth)\n",
    "            ]\n",
    "        )\n",
    "        self.downsample = downsample\n",
    "        if callable(self.downsample):\n",
    "            self.downsample = downsample(dim=dim, norm_layer=norm_layer, spatial_dims=len(self.window_size))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_shape = x.size()\n",
    "        if len(x_shape) == 5:\n",
    "            b, c, d, h, w = x_shape\n",
    "            window_size, shift_size = get_window_size((d, h, w), self.window_size, self.shift_size)\n",
    "            x = rearrange(x, \"b c d h w -> b d h w c\")\n",
    "            dp = int(np.ceil(d / window_size[0])) * window_size[0]\n",
    "            hp = int(np.ceil(h / window_size[1])) * window_size[1]\n",
    "            wp = int(np.ceil(w / window_size[2])) * window_size[2]\n",
    "            attn_mask = compute_mask([dp, hp, wp], window_size, shift_size, x.device)\n",
    "            for blk in self.blocks:\n",
    "                x = blk(x, attn_mask)\n",
    "            x = x.view(b, d, h, w, -1)\n",
    "            if self.downsample is not None:\n",
    "                x = self.downsample(x)\n",
    "            x = rearrange(x, \"b d h w c -> b c d h w\")\n",
    "\n",
    "        elif len(x_shape) == 4:\n",
    "            b, c, h, w = x_shape\n",
    "            window_size, shift_size = get_window_size((h, w), self.window_size, self.shift_size)\n",
    "            x = rearrange(x, \"b c h w -> b h w c\")\n",
    "            hp = int(np.ceil(h / window_size[0])) * window_size[0]\n",
    "            wp = int(np.ceil(w / window_size[1])) * window_size[1]\n",
    "            attn_mask = compute_mask([hp, wp], window_size, shift_size, x.device)\n",
    "            for blk in self.blocks:\n",
    "                x = blk(x, attn_mask)\n",
    "            x = x.view(b, h, w, -1)\n",
    "            if self.downsample is not None:\n",
    "                x = self.downsample(x)\n",
    "            x = rearrange(x, \"b h w c -> b c h w\")\n",
    "        return x\n",
    "\n",
    "\n",
    "class SwinTransformer(nn.Module):\n",
    "    \"\"\"\n",
    "    Swin Transformer based on: \"Liu et al.,\n",
    "    Swin Transformer: Hierarchical Vision Transformer using Shifted Windows\n",
    "    <https://arxiv.org/abs/2103.14030>\"\n",
    "    https://github.com/microsoft/Swin-Transformer\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_chans: int,\n",
    "        embed_dim: int,\n",
    "        window_size: Sequence[int],\n",
    "        patch_size: Sequence[int],\n",
    "        depths: Sequence[int],\n",
    "        num_heads: Sequence[int],\n",
    "        mlp_ratio: float = 4.0,\n",
    "        qkv_bias: bool = True,\n",
    "        drop_rate: float = 0.0,\n",
    "        attn_drop_rate: float = 0.0,\n",
    "        drop_path_rate: float = 0.0,\n",
    "        norm_layer: type[LayerNorm] = nn.LayerNorm,\n",
    "        patch_norm: bool = False,\n",
    "        use_checkpoint: bool = False,\n",
    "        spatial_dims: int = 3,\n",
    "        downsample=\"merging\",\n",
    "        use_v2=False,\n",
    "        return_all_tokens=False,\n",
    "        masked_im_modeling=False\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            in_chans: dimension of input channels.\n",
    "            embed_dim: number of linear projection output channels.\n",
    "            window_size: local window size.\n",
    "            patch_size: patch size.\n",
    "            depths: number of layers in each stage.\n",
    "            num_heads: number of attention heads.\n",
    "            mlp_ratio: ratio of mlp hidden dim to embedding dim.\n",
    "            qkv_bias: add a learnable bias to query, key, value.\n",
    "            drop_rate: dropout rate.\n",
    "            attn_drop_rate: attention dropout rate.\n",
    "            drop_path_rate: stochastic depth rate.\n",
    "            norm_layer: normalization layer.\n",
    "            patch_norm: add normalization after patch embedding.\n",
    "            use_checkpoint: use gradient checkpointing for reduced memory usage.\n",
    "            spatial_dims: spatial dimension.\n",
    "            downsample: module used for downsampling, available options are `\"mergingv2\"`, `\"merging\"` and a\n",
    "                user-specified `nn.Module` following the API defined in :py:class:`monai.networks.nets.PatchMerging`.\n",
    "                The default is currently `\"merging\"` (the original version defined in v0.9.0).\n",
    "            use_v2: using swinunetr_v2, which adds a residual convolution block at the beginning of each swin stage.\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "        self.num_layers = len(depths)\n",
    "        self.embed_dim = embed_dim\n",
    "        self.patch_norm = patch_norm\n",
    "        self.window_size = window_size\n",
    "        self.patch_size = patch_size\n",
    "        # self.patch_embed = PatchEmbed(\n",
    "        #     patch_size=(1, 1, 1),\n",
    "        #     in_chans=in_chans,\n",
    "        #     embed_dim=embed_dim,\n",
    "        #     norm_layer=norm_layer if self.patch_norm else None,  # type: ignore\n",
    "        #     spatial_dims=spatial_dims,\n",
    "        # )\n",
    "        self.patch_embed = PatchEmbed(\n",
    "            patch_size=self.patch_size,\n",
    "            in_chans=in_chans,\n",
    "            embed_dim=embed_dim,\n",
    "            norm_layer=norm_layer if self.patch_norm else None,  # type: ignore\n",
    "            spatial_dims=spatial_dims,\n",
    "        )\n",
    "        self.pos_drop = nn.Dropout(p=drop_rate)\n",
    "        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, sum(depths))]\n",
    "        self.use_v2 = use_v2\n",
    "        self.layers1 = nn.ModuleList()\n",
    "        self.layers2 = nn.ModuleList()\n",
    "        self.layers3 = nn.ModuleList()\n",
    "        self.layers4 = nn.ModuleList()\n",
    "        \n",
    "        self.mlplayer0=MLPBlockSwin(48,256)\n",
    "        self.mlplayer1=MLPBlockSwin(96,256)\n",
    "        self.mlplayer2=MLPBlockSwin(192,256)\n",
    "        self.mlplayer3=MLPBlockSwin(384,256)\n",
    "        \n",
    "        if self.use_v2:\n",
    "            self.layers1c = nn.ModuleList()\n",
    "            self.layers2c = nn.ModuleList()\n",
    "            self.layers3c = nn.ModuleList()\n",
    "            self.layers4c = nn.ModuleList()\n",
    "        down_sample_mod = look_up_option(downsample, MERGING_MODE) if isinstance(downsample, str) else downsample\n",
    "        for i_layer in range(self.num_layers):\n",
    "            layer = BasicLayer(\n",
    "                dim=int(embed_dim * 2**i_layer),\n",
    "                depth=depths[i_layer],\n",
    "                num_heads=num_heads[i_layer],\n",
    "                window_size=self.window_size,\n",
    "                drop_path=dpr[sum(depths[:i_layer]) : sum(depths[: i_layer + 1])],\n",
    "                mlp_ratio=mlp_ratio,\n",
    "                qkv_bias=qkv_bias,\n",
    "                drop=drop_rate,\n",
    "                attn_drop=attn_drop_rate,\n",
    "                norm_layer=norm_layer,\n",
    "                downsample=down_sample_mod if (i_layer < self.num_layers - 1) else None,\n",
    "                use_checkpoint=use_checkpoint,\n",
    "            )\n",
    "            if i_layer == 0:\n",
    "                self.layers1.append(layer)\n",
    "            elif i_layer == 1:\n",
    "                self.layers2.append(layer)\n",
    "            elif i_layer == 2:\n",
    "                self.layers3.append(layer)\n",
    "            elif i_layer == 3:\n",
    "                self.layers4.append(layer)\n",
    "            if self.use_v2:\n",
    "                layerc = UnetrBasicBlock(\n",
    "                    spatial_dims=spatial_dims,\n",
    "                    in_channels=embed_dim * 2**i_layer,\n",
    "                    out_channels=embed_dim * 2**i_layer,\n",
    "                    kernel_size=3,\n",
    "                    stride=1,\n",
    "                    norm_name=\"instance\",\n",
    "                    res_block=True,\n",
    "                )\n",
    "                if i_layer == 0:\n",
    "                    self.layers1c.append(layerc)\n",
    "                elif i_layer == 1:\n",
    "                    self.layers2c.append(layerc)\n",
    "                elif i_layer == 2:\n",
    "                    self.layers3c.append(layerc)\n",
    "                elif i_layer == 3:\n",
    "                    self.layers4c.append(layerc)\n",
    "                    \n",
    "        self.num_features = int(embed_dim * 2 ** (self.num_layers - 1))\n",
    "        self.return_all_tokens = return_all_tokens\n",
    "        self.norm = norm_layer(self.num_features)\n",
    "        self.avgpool = nn.AdaptiveAvgPool1d(1)\n",
    "        \n",
    "        # masked image modeling\n",
    "        self.masked_im_modeling = masked_im_modeling\n",
    "        if masked_im_modeling:\n",
    "            self.masked_embed = nn.Parameter(torch.zeros(1, embed_dim))\n",
    "\n",
    "    def proj_out(self, x, normalize=False):\n",
    "        if normalize:\n",
    "            x_shape = x.shape\n",
    "            # Force trace() to generate a constant by casting to int\n",
    "            ch = int(x_shape[1])\n",
    "            if len(x_shape) == 5:\n",
    "                x = rearrange(x, \"n c d h w -> n d h w c\")\n",
    "                x = F.layer_norm(x, [ch])\n",
    "                x = rearrange(x, \"n d h w c -> n c d h w\")\n",
    "            elif len(x_shape) == 4:\n",
    "                x = rearrange(x, \"n c h w -> n h w c\")\n",
    "                x = F.layer_norm(x, [ch])\n",
    "                x = rearrange(x, \"n h w c -> n c h w\")\n",
    "        return x\n",
    "\n",
    "    def forward(self, x, normalize=True, return_all_tokens=None, mask=None):\n",
    "        x0 = self.patch_embed(x)\n",
    "        # x_out = self.proj_out(x, normalize)\n",
    "        # x0 = self.patch_embed1(x_out)\n",
    "        # mask image modeling\n",
    "        if mask is not None:\n",
    "            x0 = self.mask_model(x0, mask)\n",
    "        \n",
    "        x0 = self.pos_drop(x0)\n",
    "        x0_out = self.proj_out(x0, normalize)\n",
    "        if self.use_v2:\n",
    "            x0 = self.layers1c[0](x0.contiguous())\n",
    "        x1 = self.layers1[0](x0.contiguous())\n",
    "        x1_out = self.proj_out(x1, normalize)\n",
    "        if self.use_v2:\n",
    "            x1 = self.layers2c[0](x1.contiguous())\n",
    "        x2 = self.layers2[0](x1.contiguous())\n",
    "        x2_out = self.proj_out(x2, normalize)\n",
    "        if self.use_v2:\n",
    "            x2 = self.layers3c[0](x2.contiguous())\n",
    "        x3 = self.layers3[0](x2.contiguous())\n",
    "        x3_out = self.proj_out(x3, normalize)\n",
    "        if self.use_v2:\n",
    "            x3 = self.layers4c[0](x3.contiguous())\n",
    "        x4 = self.layers4[0](x3.contiguous())\n",
    "        x4_out = self.proj_out(x4, normalize)\n",
    "        \n",
    "        B, C, D, H, W = x4_out.shape\n",
    "        x = x4_out.permute(0, 2, 3, 4, 1).reshape(B, D*H*W, C)\n",
    "        x_region = self.norm(x)  # B L C\n",
    "        x = self.avgpool(x_region.transpose(1, 2))  # B C 1\n",
    "        x = torch.flatten(x, 1)\n",
    "\n",
    "        return_all_tokens = self.return_all_tokens if \\\n",
    "            return_all_tokens is None else return_all_tokens\n",
    "        if return_all_tokens:\n",
    "            return torch.cat([x.unsqueeze(1), x_region], dim=1)\n",
    "\n",
    "        x0_out=self.mlplayer0(x0_out)\n",
    "        x1_out=self.mlplayer1(x1_out)\n",
    "        x2_out=self.mlplayer2(x2_out)\n",
    "        x3_out=self.mlplayer3(x3_out)\n",
    "        x_out=torch.concatenate((x0_out,x1_out,x2_out,x3_out),axis=1)\n",
    "        return [x_out]\n",
    "    \n",
    "    def mask_model(self, x, mask):\n",
    "        # extend mask for hierarchical features\n",
    "        if x.shape[-3:] != mask.shape[-3:]:\n",
    "            dtimes, htimes, wtimes = np.array(x.shape[-3:]) // np.array(mask.shape[-3:])\n",
    "            mask = mask.repeat_interleave(dtimes, -3).repeat_interleave(htimes, -2).repeat_interleave(wtimes, -1)\n",
    "        \n",
    "        # mask embed\n",
    "        x.permute(0, 2, 3, 4, 1)[mask, :] = self.masked_embed.to(x.dtype)\n",
    "\n",
    "        return x\n",
    "    \n",
    "class MLPBlockSwin(nn.Module):\n",
    "    \"\"\"\n",
    "    A multi-layer perceptron block, based on: \"Dosovitskiy et al.,\n",
    "    An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale <https://arxiv.org/abs/2010.11929>\"\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, input_dim: int, hidden_size: int, dropout_rate: float = 0.0, act: tuple | str = \"GELU\", dropout_mode=\"vit\"\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            hidden_size: dimension of hidden layer.\n",
    "            input_dim: dimension of input layer.\n",
    "            mlp_dim: dimension of feedforward layer. If 0, `hidden_size` will be used.\n",
    "            dropout_rate: fraction of the input units to drop.\n",
    "            act: activation type and arguments. Defaults to GELU. Also supports \"GEGLU\" and others.\n",
    "            dropout_mode: dropout mode, can be \"vit\" or \"swin\".\n",
    "                \"vit\" mode uses two dropout instances as implemented in\n",
    "                https://github.com/google-research/vision_transformer/blob/main/vit_jax/models.py#L87\n",
    "                \"swin\" corresponds to one instance as implemented in\n",
    "                https://github.com/microsoft/Swin-Transformer/blob/main/models/swin_mlp.py#L23\n",
    "                \"vista3d\" mode does not use dropout.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        if not (0 <= dropout_rate <= 1):\n",
    "            raise ValueError(\"dropout_rate should be between 0 and 1.\")\n",
    "        self.linear1 = nn.Linear(input_dim, hidden_size) \n",
    "        self.linear2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fn = get_act_layer(act)\n",
    "        # Use Union[nn.Dropout, nn.Identity] for type annotations\n",
    "        self.drop1: Union[nn.Dropout, nn.Identity]\n",
    "        self.drop2: Union[nn.Dropout, nn.Identity]\n",
    "\n",
    "        dropout_opt = look_up_option(dropout_mode, SUPPORTED_DROPOUT_MODE)\n",
    "        if dropout_opt == \"vit\":\n",
    "            self.drop1 = nn.Dropout(dropout_rate)\n",
    "            self.drop2 = nn.Dropout(dropout_rate)\n",
    "        elif dropout_opt == \"swin\":\n",
    "            self.drop1 = nn.Dropout(dropout_rate)\n",
    "            self.drop2 = self.drop1\n",
    "        elif dropout_opt == \"vista3d\":\n",
    "            self.drop1 = nn.Identity()\n",
    "            self.drop2 = nn.Identity()\n",
    "        else:\n",
    "            raise ValueError(f\"dropout_mode should be one of {SUPPORTED_DROPOUT_MODE}\")\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, C, D, H, W = x.shape\n",
    "        x=x.permute(0, 2, 3, 4, 1).reshape(B, D*H*W, C) # B,C,D,H,W -> B,D*H*W,C \n",
    "        x = self.fn(self.linear1(x))\n",
    "        x = self.drop1(x)\n",
    "        x = self.linear2(x)\n",
    "        x = self.drop2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "def swin_3D(window_size=(7, 7, 7), **kwargs):\n",
    "    model = SwinTransformer(\n",
    "            in_chans=1,\n",
    "            embed_dim=48,\n",
    "            window_size=window_size,\n",
    "            patch_size=(4, 4, 4),\n",
    "            depths=[2, 2, 2, 2],\n",
    "            num_heads=[3, 6, 12, 24],\n",
    "            mlp_ratio=4.0,\n",
    "            qkv_bias=True,\n",
    "            drop_rate=0.0,\n",
    "            attn_drop_rate=0.0,\n",
    "            drop_path_rate=0.0,\n",
    "            norm_layer=torch.nn.LayerNorm,\n",
    "            spatial_dims=3,\n",
    "            **kwargs\n",
    "        )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "4659b468-8fa4-41ed-854c-0b639b25ae20",
   "metadata": {},
   "outputs": [],
   "source": [
    "student = swin_3D()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "759b362d-33b9-4186-992b-6d1986dec9bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_input=torch.Tensor(np.ones((2,1,64,32,32)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "07e3b1e9-a77d-4c38-b66b-03b8ce5728eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 48, 16, 8, 8)\n",
      "(2, 96, 8, 4, 4)\n",
      "(2, 192, 4, 2, 2)\n",
      "(2, 384, 2, 1, 1)\n",
      "(2, 384, 2, 1, 1)\n"
     ]
    }
   ],
   "source": [
    "result=student(test_input)\n",
    "for i in result:\n",
    "    print(np.shape(i.detach().numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "ff29b4a2-53f5-4901-9c8a-4190877cc701",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 1170, 256)\n"
     ]
    }
   ],
   "source": [
    "result=student(test_input)\n",
    "for i in result:\n",
    "    print(np.shape(i.detach().numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "da7c7f30-81b9-4d76-a4b8-210d31ebb87f",
   "metadata": {},
   "outputs": [],
   "source": [
    "xd=torch.Tensor(np.ones((16,32,32,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "867006d5-c1a0-483b-b2a9-75f6cee8bc2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 2048])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xd2=xd.reshape((16,-1))\n",
    "xd2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "e4c3a816-7caa-4836-8fb8-bbb6f86b8360",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1170"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "16*8*8+8*4*4+4*2*2+2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "a539360a-9979-4a9f-87bd-38611edda9c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "ones=torch.Tensor(np.ones((1,1028,256)))\n",
    "twos=torch.Tensor(np.ones((1,1028,256))*2)\n",
    "ow=torch.concatenate((ones,twos))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "68bb53ce-81cf-45a6-bfd0-479d25ecc578",
   "metadata": {},
   "outputs": [],
   "source": [
    "threes=torch.Tensor(np.ones((1,128,256))*3)\n",
    "fours=torch.Tensor(np.ones((1,128,256))*4)\n",
    "tf=torch.concatenate((threes,fours))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "3ff43940-5b10-4521-a9af-10645b54f306",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 128, 256])"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "f2afbc28-0282-4140-99d4-8763eb5c7e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "owtf=torch.concatenate((ow,tf),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "69935543-b291-495b-9016-fbaa17b1810e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1., 3.], dtype=float32), array([263168,  32768]))"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(owtf[0],return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "74e57aed-6eaf-4502-81c9-88619b434a57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32768"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "128*256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75e0781e-072c-458b-a5fd-b2fde7958a7d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ctencoders",
   "language": "python",
   "name": "ctencoders"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
